import openai
from openai import OpenAI
import streamlit as st
import json
import pandas as pd
import io
import re
import requests
from bs4 import BeautifulSoup


from urllib.parse import urljoin, urlparse
import hashlib

# Google Ads imports
from google.ads.googleads.client import GoogleAdsClient
from google.ads.googleads.errors import GoogleAdsException

# --- Stable hash of inputs to detect changes between analyse og generering ---
def compute_input_hash(xpect_text, customer_website, additional_info, geo_areas, campaign_types, budget) -> str:
    payload = json.dumps({
        "x": xpect_text or "",
        "w": customer_website or "",
        "a": additional_info or "",
        "g": geo_areas or "",
        "t": campaign_types or [],
        "b": budget,
    }, ensure_ascii=False, sort_keys=True)
    return hashlib.md5(payload.encode("utf-8")).hexdigest()

# --- Simple domain extraction (no external deps) ---
DOMAIN_REGEX = r"[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b"

def extract_domains(text: str) -> list:
    """Extract plausible root domains (e.g., 'invita.dk') from arbitrary text.
    Lightweight, no external deps. Strips scheme/www and returns last two labels."""
    if not text:
        return []
    candidates = set()
    for token in re.findall(DOMAIN_REGEX, text or ""):
        token = token.lower().strip()
        token = token.replace("http://", "").replace("https://", "")
        # hostname only
        token = token.split("/")[0]
        token = token.lstrip(".")
        token = token.replace("www.", "")
        labels = [p for p in token.split(".") if p]
        if len(labels) >= 2:
            root = ".".join(labels[-2:])
            candidates.add(root)
    return list(candidates)

def get_external_domains_from_homepage(base_url: str, max_domains: int = 5) -> list:
    """Fetch homepage and collect external link root domains as a fallback."""
    try:
        resp = requests.get(base_url, timeout=6)
        s = BeautifulSoup(resp.text, "html.parser")
        base_domain = urlparse(base_url).netloc.replace("www.", "").lower()
        outs = set()
        for a in s.find_all("a", href=True):
            href = a["href"]
            full = urljoin(base_url, href)
            host = urlparse(full).netloc.lower()
            host = host.replace("www.", "")
            if host and base_domain not in host and host != base_domain:
                outs.update(extract_domains(full))
        # prune
        outs = [d for d in outs if d and d != base_domain]
        return sorted(outs)[:max_domains]
    except Exception:
        return []

st.set_page_config(page_title="Google Ads ‚Äì Kampagne Generator", layout="wide")
st.title("Google Ads ‚Äì Kampagne Generator")

st.header("1. Inds√¶t Xpect")

# Mulighed for b√•de upload og inds√¶t Xpect
uploaded_xpect = st.file_uploader("Upload Xpect (PDF, TXT eller DOCX)", type=["pdf", "txt", "docx"])
if uploaded_xpect is not None:
    if uploaded_xpect.type == "application/pdf":
        import PyPDF2
        reader = PyPDF2.PdfReader(uploaded_xpect)
        xpect_text = "\n".join(page.extract_text() or "" for page in reader.pages)
    elif uploaded_xpect.type in ["application/vnd.openxmlformats-officedocument.wordprocessingml.document", "application/msword"]:
        from docx import Document
        doc = Document(uploaded_xpect)
        xpect_text = "\n".join([p.text for p in doc.paragraphs])
    else:
        xpect_text = uploaded_xpect.read().decode("utf-8", errors="ignore")
    st.success("‚úÖ Xpect-fil uploadet")
else:
    xpect_text = st.text_area("Kopier hele Xpect-teksten her")

# --- Geografisk omr√•deudtr√¶k fra Xpect ---
def extract_cities_from_xpect(xpect_text):
    """Find simple Danish city names in Xpect text."""
    city_list = [
        "K√∏benhavn","Aarhus","√Örhus","Odense","Aalborg","Esbjerg","Randers","Kolding","Horsens","Vejle","Roskilde",
        "Herning","H√∏rsholm","Silkeborg","N√¶stved","Fredericia","Viborg","K√∏ge","Holstebro","Taastrup","Slagelse",
        "Hadsund","Hobro","Skanderborg","Grenaa","Ebeltoft","Ikast","Svendborg","Ringk√∏bing","Hiller√∏d","S√∏nderborg",
        "Helsing√∏r","Kalundborg","Thisted","Frederikshavn","Nyk√∏bing","Holb√¶k","Nakskov","Middelfart","R√∏nne","Aabenraa"
    ]
    found = []
    text = xpect_text or ""
    for c in city_list:
        # Correct word-boundary regex (use re.escape and single \b)
        if re.search(rf"\b{re.escape(c)}\b", text, re.IGNORECASE):
            # Normalize √Örhus/Aarhus to Aarhus, and title-case others
            norm = "Aarhus" if c in ("Aarhus", "√Örhus") else c
            found.append(norm)
    # Unique + title case output
    return sorted(set([f.title() for f in found]))

geo_areas_default = ""
placeholder_text = ""
if xpect_text:
    cities = extract_cities_from_xpect(xpect_text)
    if cities:
        geo_areas_default = ", ".join(cities)
    else:
        placeholder_text = "Ingen geografier fundet i Xpect"
        st.info("‚ÑπÔ∏è Ingen geografier fundet i Xpect. Du kan skrive dem manuelt (fx 'Randers, Hobro').")
geo_areas = st.text_input(
    "Geografiske omr√•der (kan redigeres)",
    value=geo_areas_default,
    placeholder=placeholder_text
)

st.header("2. Kundens website")
customer_website = st.text_input("Indtast URL til kundens website")

def get_internal_links(base_url, soup):
    base_domain = urlparse(base_url).netloc
    links = set()
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        full_url = urljoin(base_url, href)
        if urlparse(full_url).netloc == base_domain:
            links.add(full_url)
    return list(links)

scraped_info = ""
# Website scraping med filtrering af relevante tags og stopord
RELEVANT_TAGS = {"h1", "h2", "h3", "title"}
STOPWORDS = {
    "forside", "om", "kontakt", "historien", "nyheder", "bliv forhandler",
    "om os", "kontakt os", "om vibegaard"
}

def clean_scraped_texts(texts, base_domain=None):
    cleaned = []
    seen = set()
    noise_pattern = re.compile(
        r"(?i)\b(nyhed|klik her|l√¶s mere|kontakt|book|campaya|snak|faq|bestil|webshop|ring|tilmelding)\b"
    )

    for t in texts:
        t_clean = re.sub(r'\s+', ' ', t.strip())
        if len(t_clean) < 15 or len(t_clean) > 120:
            continue
        if noise_pattern.search(t_clean):
            continue
        if base_domain:
            base = base_domain.split('.')[0]
            if base in t_clean.lower():
                continue
        if "|" in t_clean:
            t_clean = t_clean.split("|")[0].strip()
        if t_clean.lower() in seen:
            continue
        seen.add(t_clean.lower())
        cleaned.append(t_clean)
    return cleaned

if customer_website:
    try:
        response = requests.get(customer_website, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")

        all_links = get_internal_links(customer_website, soup)
        pages_to_scrape = [customer_website] + all_links[:4]

        all_texts = []
        for url in pages_to_scrape:
            try:
                res = requests.get(url, timeout=5)
                sub_soup = BeautifulSoup(res.text, "html.parser")
                headings = [h.get_text(strip=True) for h in sub_soup.find_all(RELEVANT_TAGS)]
                all_texts.extend(headings)
            except Exception:
                continue

        filtered = clean_scraped_texts(all_texts, base_domain=urlparse(customer_website).netloc)
        scraped_info = "\n".join(filtered[:50])
        st.write("üîé Website analyseret ‚Äì fundet f√∏lgende relevante indhold:")
        st.code(scraped_info)
    except Exception as e:
        st.warning(f"Kunne ikke analysere website: {e}")

st.header("3. Samlet dagsbudget")
total_daily_budget = st.number_input("Indtast samlet dagsbudget for alle kampagner (DKK)", min_value=1, value=500)

st.header("4. √ònskede kampagnetyper")
selected_campaign_types = st.multiselect(
    "V√¶lg kampagnetyper du √∏nsker at inkludere",
    ["Search", "Display", "Performance Max", "Shopping", "Video"],
    default=["Search"]
)

st.header("5. Anden vigtig information")
additional_info = st.text_area("Skriv evt. yderligere info, som ikke st√•r i Xpect")

api_key = st.sidebar.text_input("Indtast din OpenAI API-n√∏gle", type="password")

# model selection
model_choice = st.sidebar.selectbox("V√¶lg model", options=["gpt-5"], index=0)

# Google Ads ‚Äì Customer ID (bruges til Keyword Planner)
gads_customer_id = st.sidebar.text_input("Google Ads customer ID (uden bindestreger)", value="7445232535")

# --- Session state defaults for analyses persistence ---
if "analysis_text" not in st.session_state:
    st.session_state["analysis_text"] = ""
if "competitor_analysis_text" not in st.session_state:
    st.session_state["competitor_analysis_text"] = ""
if "competitor_input" not in st.session_state:
    st.session_state["competitor_input"] = ""
if "analyses_ready" not in st.session_state:
    st.session_state["analyses_ready"] = False
if "analysis_hash" not in st.session_state:
    st.session_state["analysis_hash"] = ""

# --- NYT TRIN 6: ANALYSEPAKKE (K√∏r samlet) ---
st.header("6. K√∏r analyser (valgfrit men anbefalet)")
st.info("üîç Indtast Xpect, website, budget, vigtig info og din API-n√∏gle f√∏r du k√∏rer analyserne.")
run_all_analyses = st.button("üöÄ K√∏r analyser")
analysis_text = ""
competitor_analysis_text = ""
if run_all_analyses and api_key and customer_website and xpect_text:
    try:
        client = OpenAI(api_key=api_key)
        # Samlet analyse prompt (foranalyse, konkurrentforslag og konkurrentanalyse i √©t kald)
        combined_analysis_prompt = f"""Du er en erfaren Google Ads strateg.
Baseret p√• nedenst√•ende input (Xpect, website, website-indhold, ekstra noter, geografiske omr√•der, √∏nskede kampagnetyper og budget), skal du udf√∏re en samlet analyse best√•ende af:

1. **Foranalyse**:
   - Prim√¶re budskaber og value proposition
   - M√•lgruppe(r)
   - USP‚Äôer
   - Tone of voice
   - Call-to-actions
   - Potentielle annoncevinkler
   Skriv kort p√• dansk i punktform.

2. **Konkurrentforslag**:
   - Find de 2‚Äì5 vigtigste konkurrenter i Danmark for virksomheden ud fra Xpect og websitet.
   - Return√©r KUN en kommasepareret liste over roddom√¶ner (uden http/https og uden understier).

3. **Konkurrentanalyse**:
   - For hver n√¶vnt konkurrent: Skriv kort p√• dansk (punktform) om deres prim√¶re budskaber, USP‚Äôer, CTA‚Äôer, tone og stil, samt eventuelle gaps.
   - Afslut med 3‚Äì5 forslag til hvordan virksomheden kan differentiere sig fra konkurrenterne.

**Outputformat**:
Return√©r √©t samlet JSON-objekt med f√∏lgende n√∏gler:
{{
  "foranalyse": "...",
  "konkurrenter": "dom√¶ne1.dk, dom√¶ne2.dk, ...",
  "konkurrentanalyse": "..."
}}

---

**Input:**
Xpect:
{xpect_text}

Website:
{customer_website}

Website-indhold:
{scraped_info}

Ekstra noter:
{additional_info}

Geografiske omr√•der:
{geo_areas}

√ònskede kampagnetyper:
{", ".join(selected_campaign_types)}

Samlet dagsbudget:
{total_daily_budget} kr.
"""
        # --- Inds√¶t spinner omkring GPT-5 kald ---
        with st.spinner("üîç Analyserer data med GPT‚Äë5..."):
            response = client.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": "Du er en erfaren Google Ads strateg, der laver foranalyse og konkurrentanalyse f√∏r kampagnestruktur."},
                    {"role": "user", "content": combined_analysis_prompt}
                ]
            )
        st.write("‚úÖ Modtog svar fra GPT-5")
        output = response.choices[0].message.content
        try:
            parsed = json.loads(output)
        except Exception:
            # fallback: find JSON in output
            m = re.search(r"\{.*\}", output, re.DOTALL)
            if m:
                try:
                    parsed = json.loads(m.group(0))
                except Exception:
                    parsed = {}
            else:
                parsed = {}
        foranalyse = parsed.get("foranalyse", "")
        konkurrenter = parsed.get("konkurrenter", "")
        konkurrentanalyse = parsed.get("konkurrentanalyse", "")
        # fallback: extract domains if konkurrenter is not a list
        suggested = extract_domains(konkurrenter)
        if not suggested:
            suggested = get_external_domains_from_homepage(customer_website, max_domains=5)
        own = urlparse(customer_website).netloc.replace("www.", "").lower()
        suggested = [d for d in suggested if d and d != own]
        st.session_state["competitor_input"] = ", ".join(sorted(set(suggested)))
        st.session_state["analysis_text"] = foranalyse
        st.session_state["competitor_analysis_text"] = konkurrentanalyse
        st.session_state["analyses_ready"] = True
        st.session_state["analysis_hash"] = compute_input_hash(
            xpect_text,
            customer_website,
            additional_info,
            geo_areas,
            selected_campaign_types,
            total_daily_budget,
        )
        st.subheader("üìä Foranalyse")
        st.write(foranalyse)
        st.subheader("üîé Foresl√•ede konkurrenter")
        st.write(st.session_state["competitor_input"])
        st.subheader("üèÅ Konkurrentanalyse")
        st.write(konkurrentanalyse)
    except Exception as e:
        st.error(f"Analyse mislykkedes: {e}")
else:
    if run_all_analyses:
        st.warning("‚ö†Ô∏è Udfyld Xpect, Website og API-n√∏gle f√∏r du k√∏rer analyserne.")

# --- FUNKTIONER SOM F√òR ---
def extract_json_from_text(text: str):
    try:
        return json.loads(text)
    except Exception:
        pass
    patterns = [r"\{.*\"campaigns\".*\}", r"\{.*\}", r"\[.*\]"]
    for pat in patterns:
        m = re.search(pat, text, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                continue
    raise ValueError("Kunne ikke finde gyldigt JSON i AI-outputtet.")

def stringify_list(items):
    if not items:
        return ""
    result = []
    for item in items:
        if isinstance(item, dict):
            text = item.get("text") or item.get("title") or str(item)
            result.append(text)
        else:
            result.append(str(item))
    return "; ".join(result)


# --- Normalize ad payloads from the model into a consistent shape ---
def normalize_ad_obj(ad: dict, customer_website: str) -> dict:
    if not isinstance(ad, dict):
        return {}
    out = dict(ad)  # shallow copy

    # Headlines may come as a list under 'headlines'
    headlines = []
    if isinstance(out.get("headlines"), list):
        headlines = out.get("headlines")
    else:
        # try to collect headline_1..headline_15 if present
        for i in range(1, 16):
            key = f"headline_{i}"
            if key in out and isinstance(out[key], str):
                headlines.append(out[key])
    for i in range(9):
        out[f"headline_{i+1}"] = (headlines[i] if i < len(headlines) else out.get(f"headline_{i+1}", ""))

    # Descriptions may come as a list under 'descriptions'
    descriptions = []
    if isinstance(out.get("descriptions"), list):
        descriptions = out.get("descriptions")
    else:
        for i in range(1, 9):
            key = f"description_{i}"
            if key in out and isinstance(out[key], str):
                descriptions.append(out[key])
    for i in range(4):
        out[f"description_{i+1}"] = (descriptions[i] if i < len(descriptions) else out.get(f"description_{i+1}", ""))

    # Final URL can be under multiple keys
    final_url = (
        out.get("final_url")
        or out.get("finalUrl")
        or (out.get("finalUrls")[0] if isinstance(out.get("finalUrls"), list) and out.get("finalUrls") else None)
        or out.get("url")
        or out.get("landing_page")
        or customer_website
    )
    out["final_url"] = final_url or customer_website or ""

    # Paths may vary
    out["path_1"] = out.get("path_1") or out.get("path1") or out.get("path") or ""
    out["path_2"] = out.get("path_2") or out.get("path2") or ""

    return out

def headline_cleanup(texts, api_key, model_choice, max_len=30, label="overskrifter"):
    """Auto-korrigerer korte eller ufuldst√¶ndige overskrifter/beskrivelser."""
    if not texts:
        return texts
    bad = [t for t in texts if re.search(r'\b(i|p√•|til|for|med)$', t.lower()) or len(t) < 15]
    if not bad:
        return texts
    prompt = f"Omskriv disse {label} til hele, men korte danske s√¶tninger. Maks {max_len} tegn, og s√¶tningen skal give mening:\n" + "\n".join(bad)
    try:
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "Du er en dansk tekstforfatter for Google Ads. Du skriver f√¶ngende, men korte s√¶tninger."},
                {"role": "user", "content": prompt}
            ]
        )
        suggestions = [s.strip() for s in resp.choices[0].message.content.split("\n") if s.strip()]
        idx = 0
        for i, t in enumerate(texts):
            if t in bad and idx < len(suggestions):
                texts[i] = suggestions[idx][:max_len].strip()
                idx += 1
    except Exception as e:
        st.warning(f"Kunne ikke forbedre {label}: {e}")
    return texts

#
# --- Google Ads Keyword Planner: hent s√∏gevolumen/CPC/konkurrence ---
def fetch_keyword_metrics(all_keywords, customer_id: str, yaml_path: str = "google-ads.yaml") -> dict:
    """Returnerer dict: { keyword_lower: {"monthly": int, "competition": str, "cpc_low": float, "cpc_high": float} }
    Bruger KeywordPlanIdeaService.GenerateKeywordIdeas.
    """
    result = {}
    keywords = [k.strip() for k in (all_keywords or []) if isinstance(k, str) and k.strip()]
    if not keywords:
        return result
    try:
        client = GoogleAdsClient.load_from_storage(yaml_path)
        service = client.get_service("KeywordPlanIdeaService")
        request = client.get_type("GenerateKeywordIdeasRequest")
        # Customer ID (uden bindestreger)
        request.customer_id = str(customer_id)
        # Dansk (1000) og Danmark (2756)
        request.language = "languageConstants/1000"
        request.geo_target_constants.append("geoTargetConstants/2756")
        request.keyword_plan_network = client.enums.KeywordPlanNetworkEnum.GOOGLE_SEARCH

        # API'en tillader op til ca. 1000 keywords ad gangen ‚Äì chunk for en sikkerheds skyld
        CHUNK = 800
        for i in range(0, len(keywords), CHUNK):
            chunk = keywords[i:i+CHUNK]
            req = client.get_type("GenerateKeywordIdeasRequest")
            req.customer_id = request.customer_id
            req.language = request.language
            req.keyword_plan_network = request.keyword_plan_network
            for g in request.geo_target_constants:
                req.geo_target_constants.append(g)
            req.keyword_seed.keywords.extend(chunk)
            response = service.generate_keyword_ideas(request=req)
            for idea in response:
                text = idea.text
                metrics = idea.keyword_idea_metrics
                if not text:
                    continue
                key = text.strip().lower()
                monthly = int(metrics.avg_monthly_searches or 0)
                comp_enum = metrics.competition  # Enum value
                competition = client.enums.KeywordPlanCompetitionLevelEnum.Name(comp_enum)
                # CPC kommer i micros (DKK * 1e6). Brug top-of-page BID low/high, hvis sat.
                low = (metrics.low_top_of_page_bid_micros or 0) / 1_000_000
                high = (metrics.high_top_of_page_bid_micros or 0) / 1_000_000
                result[key] = {
                    "monthly": monthly,
                    "competition": competition.title() if isinstance(competition, str) else str(competition),
                    "cpc_low": round(low, 2),
                    "cpc_high": round(high, 2),
                }
        return result
    except FileNotFoundError:
        st.warning("üòï Fandt ikke `google-ads.yaml` i projektmappen. Opret den f√∏rst.")
    except GoogleAdsException as ex:
        st.error(f"Google Ads API fejl: {ex}")
    except Exception as e:
        st.error(f"Kunne ikke hente s√∏gevolumen: {e}")
    return result

# --- KAMPAGNEGENERERING ---
if st.button("Gem og forts√¶t"):
    # --- Validate analyses state and show helpful hints ---
    current_hash = compute_input_hash(
        xpect_text,
        customer_website,
        additional_info,
        geo_areas,
        selected_campaign_types,
        total_daily_budget,
    )
    if st.session_state.get("analysis_hash") and st.session_state["analysis_hash"] != current_hash:
        st.info("‚ö†Ô∏è Du har √¶ndret input siden sidste analyse. Overvej at k√∏re analyserne igen for bedst resultat.")
    if not st.session_state.get("analyses_ready"):
        st.warning("Tip: K√∏r analyser f√∏rst for skarpere output (jeg kan godt forts√¶tte uden, men kvaliteten bliver bedre med analyserne).")
    input_data = {
        "xpect": xpect_text,
        "website": customer_website,
        "extra": additional_info,
        "budget": total_daily_budget,
        "campaign_types": selected_campaign_types,
        "analysis": st.session_state.get("analysis_text", ""),
        "competitor_analysis": st.session_state.get("competitor_analysis_text", ""),
    }

    with st.spinner("ü§ñ AI arbejder p√• at analysere input og oprette kampagnestruktur..."):
        system_prompt = """\
Create a detailed Google Ads campaign structure for a new client using the provided requirements specification (Xpect), URL, analysis, and extra notes as input. Your tasks are to:

- Propose the following components:
  - **Campaign structure:** Specify type(s) (Search/Display), and note if it's brand vs. generic.
  - **Campaign names**
  - **Ad groups** (with clear theme and focus)
  - **Keywords:** Up to 10 for each ad group
  - **Daily budget** for each campaign
  - **One ad per ad group** with at least 9 unique, high-quality Danish headlines and 4 descriptions.
  - **Campaign extensions:** Always include exactly 4 sitelinks, 3-4 callout extensions, structured snippets, and call extensions.

#### Important ad copy guidelines (headlines & descriptions):

- Write short, complete sentences (20‚Äì30 characters per headline).
- Do not truncate in the middle of words.
- All headlines must be full, meaningful sentences for users.
- **Never end** a headline or description with a single Danish function word: ‚Äúi‚Äù, ‚Äúp√•‚Äù, ‚Äútil‚Äù, ‚Äúfor‚Äù, ‚Äúmed‚Äù, ‚Äúom‚Äù, ‚Äúaf‚Äù, ‚Äúhos‚Äù, ‚Äúved‚Äù, ‚Äúuden‚Äù, ‚Äúover‚Äù, ‚Äúunder‚Äù, ‚Äúmellem‚Äù.
- Use natural, flowing, and professional Danish as seen in real Google Ads.
- Only capitalize at the beginning of sentences or for proper nouns.
- Descriptions (max 90 characters) must be full, meaningful sentences‚Äînever cut off or incomplete.
- Avoid repetition, filler, or partial sentences.

**Good headline examples (under 30 characters):**
- ‚ÄúFlyt nemt og sikkert‚Äù
- ‚ÄúF√• gratis flyttetilbud‚Äù
- ‚ÄúTransport i hele Jylland‚Äù
- ‚ÄúProfessionel flyttehj√¶lp‚Äù

**Good description examples:**
- ‚ÄúVi tilbyder tryg og hurtig flytning i hele Danmark.‚Äù
- ‚ÄúF√• et gratis tilbud og flyt uden besv√¶r.‚Äù
- ‚ÄúErfaren flyttehj√¶lp til private og erhverv.‚Äù

---

### Output Format

- Output MUST be in JSON format with the top key: **"campaigns"**
- Each campaign must include: campaign type, name, daily budget, ad groups (with name, theme, keywords), ad (with 9+ headlines, 4+ descriptions), and extensions (sitelinks, callouts, structured snippets, call).
- Do NOT add explanations or text outside the JSON structure.

---

### Reasoning and Output Ordering

1. **Reasoning and Drafting:** Internally, first analyze all input information (brief/specification, URL, analysis, extra notes), select relevant themes, and define a logical account and campaign structure. Then, for each element (ad groups, keywords, extensions, etc.), determine optimal content based on the input. Ensure all ad text rules are followed and content is audience-appropriate.
2. **Conclusion/Output:** Only after the complete structure and copy are finalized, output the full JSON as described.

**NB:** Your output should consist exclusively of the final ‚Äúcampaigns‚Äù JSON, meeting all detailed requirements.

---

### Example (shortened for illustration only; real output should be longer and more detailed):

#### Input:
- Xpect: [Kravspecifikation placeholder]
- URL: [www.flytteservice.dk]
- Analyse: [M√•lgruppen er private og mindre erhverv, fokus p√• lokal service.]
- Noter: [√ònsker at fremst√• professionel og hj√¶lpsom.]

#### Output:
{
  "campaigns": [
    {
      "type": "Search",
      "name": "Flytning Privat",
      "daily_budget": 500,
      "ad_groups": [
        {
          "name": "Flytning tilbud",
          "theme": "Tilbud og pris p√• flytning",
          "keywords": ["flyttetilbud", "pris p√• flytning", "..."],
          "ad": {
            "headlines": [
              "F√• gratis flyttetilbud",
              "Flyt trygt og nemt",
              "...(mindst 9 i alt)..."
            ],
            "descriptions": [
              "F√• et gratis tilbud og flyt uden besv√¶r.",
              "Erfarent hold hj√¶lper dig hele vejen.",
              "...(mindst 4 i alt)..."
            ]
          }
        }
        // ... fl. annoncegrupper ...
      ],
      "extensions": {
        "sitelinks": ["Om os", "Kontakt", "Referencer", "Priser"],
        "callouts": ["Gratis tilbud", "Erfaren flyttemand", "..."],
        "structured_snippets": ["Byer: K√∏benhavn, Aarhus, Odense"],
        "call": ["+45 12 34 56 78"]
      }
    }
    // ... fl. kampagner ...
  ]
}

---

#### Edge Cases & Special Notes

- If the input includes branding or generic strategies, ensure campaigns reflect this split.
- If you need to invent plausible ad copy, make sure everything meets the stated ad guidelines.
- Use placeholders for sensitive or variable input data (e.g., [VIRKSOMHEDSNAVN], [TELEFONNUMMER]).
- If the input specification is sparse, draft the most logical Google Ads campaign structure possible.

---

**REMINDER:** Your task is to produce only the campaigns JSON per the structure above, following all copy/extension rules. Think step-by-step before outputting the final result.
"""

        analysis_for_prompt = st.session_state.get("analysis_text", "")
        competitor_analysis_for_prompt = st.session_state.get("competitor_analysis_text", "")
        user_prompt = f"""\
Xpect:
{xpect_text}

Website:
{customer_website}

Analyse (fra AI):
{analysis_for_prompt}

Konkurrentanalyse (fra AI):
{competitor_analysis_for_prompt}

Ekstra noter:
{additional_info}

Geografisk fokus:
{geo_areas}

Website-indhold fundet ved scanning:
{scraped_info}

√ònskede kampagnetyper: {", ".join(selected_campaign_types)}.
Samlet dagsbudget m√• ikke overstige {total_daily_budget} kr.
Lav kampagnestrukturen s√• der laves kampagner eller annoncegrupper for alle n√¶vnte geografiske omr√•der. 
Hvis budgettet er lavt, fordel det j√¶vnt mellem omr√•derne.
"""

        try:
            if not api_key:
                st.error("Indtast din OpenAI API-n√∏gle i sidebaren for at k√∏re AI-analysen.")
                raise RuntimeError("Missing API key")

            client = OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            output_text = response.choices[0].message.content

            try:
                data = extract_json_from_text(output_text)
            except ValueError as e:
                st.error(str(e))
                data = None

            if not data:
                st.info("AI genererede ingen gyldig kampagnestruktur.")
                st.stop()

            # --- Hent s√∏gevolumen/CPC/konkurrence for alle keywords ---
            all_keywords = []
            for c in data.get("campaigns", []):
                for ag in c.get("ad_groups", []) or []:
                    all_keywords.extend((ag.get("keywords") or []))
            unique_keywords = sorted(set([k.strip() for k in all_keywords if isinstance(k, str) and k.strip()]))

            metrics_map = {}
            df_kw = None
            if unique_keywords and gads_customer_id:
                st.subheader("üîé Henter s√∏gevolumen fra Keyword Planner‚Ä¶")
                metrics_map = fetch_keyword_metrics(unique_keywords, gads_customer_id)
                if metrics_map:
                    # Vis som tabel i UI
                    table_rows = []
                    for kw in unique_keywords:
                        m = metrics_map.get(kw.lower(), {})
                        table_rows.append({
                            "Keyword": kw,
                            "Monthly Searches": m.get("monthly", 0),
                            "Competition": m.get("competition", ""),
                            "Top of page bid (low)": m.get("cpc_low", 0.0),
                            "Top of page bid (high)": m.get("cpc_high", 0.0),
                        })
                    df_kw = pd.DataFrame(table_rows)
                    st.dataframe(df_kw, use_container_width=True)
                    # Add download button for keyword data
                    st.download_button(
                        label="Download s√∏geordsdata (CSV)",
                        data=df_kw.to_csv(index=False, sep="\t", encoding="utf-16", lineterminator="\n"),
                        file_name="keyword_volume_data.csv",
                        mime="text/csv"
                    )
                else:
                    st.info("Ingen s√∏gevolumendata returneret ‚Äì forts√¶tter uden filtrering.")
            else:
                st.info("Ingen keywords fundet eller mangler Google Ads customer ID ‚Äì forts√¶tter uden s√∏gevolumen.")

            rows = []
            # --- Helper functions for asset cleaning ---
            TRAILING_STOPWORDS = {
                "i","p√•","til","for","med","om","af","hos","ved","uden","over","under","mellem"
            }

            def soft_trim(text: str, max_len: int) -> str:
                if not text:
                    return ""
                t = text.strip()
                if len(t) <= max_len:
                    return t
                cut = t[:max_len]
                if " " in cut:
                    cut = cut.rsplit(" ", 1)[0]
                return cut.strip()

            def polish_line(text: str, max_len: int) -> str:
                """G√∏r linjen l√¶sbar uden at klippe ord og uden h√¶ngende funktionsord."""
                if not text:
                    return ""
                t = soft_trim(text, max_len)
                t = t.strip(" -¬∑‚Ä¢,;:")
                parts = t.split()
                if parts and parts[-1].lower() in TRAILING_STOPWORDS:
                    t = " ".join(parts[:-1]).strip()
                return t

            def clean_asset(text, m):
                if not text:
                    return ""
                return polish_line(text, m)

            for campaign in data.get("campaigns", []):
                campaign_name = campaign.get("name", "")
                campaign_type = campaign.get("type", "")
                budget = campaign.get("daily_budget", "")
                sitelinks = campaign.get("sitelinks", [])
                callouts = campaign.get("callouts", [])
                structured_snippets = campaign.get("structured_snippets", [])
                call_extensions = campaign.get("call_extensions", [])

                for ad_group in campaign.get("ad_groups", []):
                    ad_group_name = ad_group.get("name", "")
                    keywords = ad_group.get("keywords", []) or []
                    ads = ad_group.get("ads", []) or []
                    ad = ads[0] if ads else {}

                    # Support alternative shapes (e.g., single 'ad' object or 'rsa')
                    if not ad and isinstance(ad_group.get("ad"), dict):
                        ad = ad_group.get("ad")
                    if not ad and isinstance(ad_group.get("rsa"), dict):
                        ad = ad_group.get("rsa")

                    # Normalize to expected keys
                    ad = normalize_ad_obj(ad, customer_website)

                    ad_headlines = [clean_asset(ad.get(f"headline_{i}", ""), 30) for i in range(1, 10)]
                    ad_descriptions = [clean_asset(ad.get(f"description_{i}", ""), 90) for i in range(1,5)]
                    final_url = ad.get("final_url", customer_website or "")

                    # Prepare pinned headlines info
                    pinned_headlines = ad.get("pinned_headlines", {}) if isinstance(ad.get("pinned_headlines"), dict) else {}

                    base_row = {
                        "Campaign": campaign_name,
                        "Campaign Type": campaign_type,
                        "Campaign Status": "Enabled",
                        # Editor expects Budget/Budget type columns
                        "Budget": budget,
                        "Budget type": "Daily",
                        # Optional but helps Editor parsing
                        "Networks": "Google search",
                        "Languages": "Danish;English",

                        "Ad Group": ad_group_name,
                        "Ad Group Status": "Enabled",

                        # Disambiguators
                        "Ad type": "Responsive search ad",
                        "Criterion Type": "",  # blank for ad rows
                        "Status": "Enabled",

                        # Keyword fields left blank for ad rows
                        "Keyword": "",
                        "Keyword Match Type": "",
                        "Keyword Status": "",

                        # Ad assets
                        "Headline 1": ad_headlines[0],
                        "Headline 2": ad_headlines[1],
                        "Headline 3": ad_headlines[2],
                        "Headline 4": ad_headlines[3],
                        "Headline 5": ad_headlines[4],
                        "Headline 6": ad_headlines[5],
                        "Headline 7": ad_headlines[6],
                        "Headline 8": ad_headlines[7],
                        "Headline 9": ad_headlines[8],
                        "Headline 1 Pin": pinned_headlines.get("headline_1", ""),
                        "Headline 2 Pin": pinned_headlines.get("headline_2", ""),
                        "Headline 3 Pin": pinned_headlines.get("headline_3", ""),
                        "Headline 4 Pin": pinned_headlines.get("headline_4", ""),
                        "Headline 5 Pin": pinned_headlines.get("headline_5", ""),
                        "Headline 6 Pin": pinned_headlines.get("headline_6", ""),
                        "Headline 7 Pin": pinned_headlines.get("headline_7", ""),
                        "Headline 8 Pin": pinned_headlines.get("headline_8", ""),
                        "Headline 9 Pin": pinned_headlines.get("headline_9", ""),
                        "Description 1": ad_descriptions[0],
                        "Description 2": ad_descriptions[1],
                        "Description 3": ad_descriptions[2],
                        "Description 4": ad_descriptions[3],
                        "Description 1 Pin": ad.get("pinned_descriptions", {}).get("description_1", ""),
                        "Description 2 Pin": ad.get("pinned_descriptions", {}).get("description_2", ""),
                        "Description 3 Pin": ad.get("pinned_descriptions", {}).get("description_3", ""),
                        "Description 4 Pin": ad.get("pinned_descriptions", {}).get("description_4", ""),
                        "Final URL": final_url,
                        "Path 1": ad.get("path_1", ""),
                        "Path 2": ad.get("path_2", ""),
                        "Sitelinks": stringify_list(sitelinks),
                        "Callouts": stringify_list(callouts),
                        "Structured Snippets": stringify_list(structured_snippets),
                        "Call Extensions": stringify_list(call_extensions),
                        "Monthly Searches": "",
                        "Competition": "",
                        "Top of page bid (low)": "",
                        "Top of page bid (high)": "",
                    }
                    rows.append(base_row)

                    # Add rows for each keyword
                    for keyword in keywords:
                        kw_row = base_row.copy()
                        kw_row.update({
                            # Disambiguators for keyword rows
                            "Ad type": "",  # must be blank for keywords
                            "Criterion Type": "Exact",
                            "Status": "Enabled",

                            "Keyword": keyword,
                            "Keyword Match Type": "Exact",
                            "Keyword Status": "Enabled",

                            # Clear ad asset fields for keyword rows
                            "Headline 1": "",
                            "Headline 2": "",
                            "Headline 3": "",
                            "Headline 4": "",
                            "Headline 5": "",
                            "Headline 6": "",
                            "Headline 7": "",
                            "Headline 8": "",
                            "Headline 9": "",
                            "Headline 1 Pin": "",
                            "Headline 2 Pin": "",
                            "Headline 3 Pin": "",
                            "Headline 4 Pin": "",
                            "Headline 5 Pin": "",
                            "Headline 6 Pin": "",
                            "Headline 7 Pin": "",
                            "Headline 8 Pin": "",
                            "Headline 9 Pin": "",
                            "Description 1": "",
                            "Description 2": "",
                            "Description 3": "",
                            "Description 4": "",
                            "Description 1 Pin": "",
                            "Description 2 Pin": "",
                            "Description 3 Pin": "",
                            "Description 4 Pin": "",
                            "Final URL": "",
                            "Path 1": "",
                            "Path 2": "",
                            "Sitelinks": "",
                            "Callouts": "",
                            "Structured Snippets": "",
                            "Call Extensions": "",
                            "Monthly Searches": (metrics_map.get(keyword.lower(), {}) or {}).get("monthly", 0),
                            "Competition": (metrics_map.get(keyword.lower(), {}) or {}).get("competition", ""),
                            "Top of page bid (low)": (metrics_map.get(keyword.lower(), {}) or {}).get("cpc_low", 0.0),
                            "Top of page bid (high)": (metrics_map.get(keyword.lower(), {}) or {}).get("cpc_high", 0.0),
                        })
                        rows.append(kw_row)

            if rows:
                df_ads = pd.DataFrame(rows)
                csv_buffer = io.BytesIO()
                df_ads.to_csv(csv_buffer, index=False, sep="\t", encoding="utf-16", lineterminator="\n")
                csv_buffer.seek(0)

                st.success("‚úÖ Kampagnestruktur genereret! Download filen nedenfor.")
                st.download_button(
                    label="Download CSV til Ads Editor",
                    data=csv_buffer,
                    file_name="ads_editor_upload.csv",
                    mime="text/csv"
                )
        except Exception as e:
            st.error(f"Fejl under AI-kald: {e}")